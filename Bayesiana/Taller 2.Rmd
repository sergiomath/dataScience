---
title: 'Taller 2: Estadística bayesiana'
author: "Sergio Andres Diaz Vera"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
##install.packages("reticulate")
knitr::opts_chunk$set(echo = FALSE)   
```
\begin{enumerate}

\item Sean $x$, $y$, y $z$ variables aleatorias con función de densidad conjunta (discreta o continua) dada por $p(x,y,z) \propto             p(x,z)p(y,z)p(z)$. Muestre que:
\begin{enumerate}
  \item $p(x\mid y,z)\propto p(x,z)$, i.e., $p(x\mid y,z)$ es función de $x$ y $z$.
  \item$p(y\mid x,z)\propto p(y,z)$, i.e., $p(y\mid x,z)$ es función de $y$ y $z$.    
  \item $x$ y $y$ son condicionalmente independientes dado $z$.
\end{enumerate}  
\item Sean $A$, $B$, y $C$ proposiciones de falso-verdadero. Suponga que $A$ y $B$ son condicionalmente independientes, dado $C$. Muestre que:
  \begin{enumerate}
    \item $A$   y $B^C$ son condicionalmente independientes, dado $C$.	
    \item $A^C$ y $B^C$ son condicionalmente independientes, dado $C$.
  \end{enumerate}
\item Sea $y\mid x\sim\textsf{Poi}(x)$ y $x\sim\textsf{Exp}(\lambda)$.
    \begin{enumerate}
       \item Muestre que la distribución marginal de $y$ es:
$$
p(y) = \frac{\lambda}{(\lambda+1)^{y+1}}\,,\qquad y = 0,1,\ldots\qquad\lambda>0\,.
$$
\textbf{Desarrollo:}

veamos que si $y\mid x\sim\textsf{Poi}(x)$  entonces $p(y\mid x)=\frac{e^{-x}x^{y}}{y!}$  y $p(x)=\lambda e^{-\lambda x}$ así
$$ p(y) = \stackrel{\text{Definición}}{\int_{\Theta} p(y,\theta)d \theta }=\int_{0}^{\infty}p(y \mid x)p(x)dx=\int_{0}^{\infty}\frac{e^{-x}x^{y}}{y!}\lambda e^{-\lambda  x}dx= \frac{\lambda}{y!}\int_{0}^{\infty} x^{y}e^{-(\lambda+1)x}dx$$ ahora que la integral toma la forma de la función de distribución de una distribución gamma la integral de esta es 1  
$$ p(y)=\frac{\lambda}{y!} \frac{\Gamma (y+1)}{(\lambda +1)^{y+1}}  \stackrel{\text{Densidad distri Gamma =1}}{ \int_{0}^{\infty} \frac{(\lambda +1)^{y+1}}{\Gamma (y+1)} x^{(y+1)-1}e^{-(\lambda+1)x}dx}=\frac{\lambda}{y!} \frac{(y)!}{(\lambda +1)^{y+1}}=\frac{\lambda}{(\lambda +1)^{y+1}}
$$
\item Simule $N=100,000$ muestras independientes e idénticamente distribuidas de $y$ con $\lambda = 1$, y compare la distribución empírica correspondiente con la distribución exacta obtenida en el numeral anterior.
\end{enumerate}
\end{enumerate}

\textbf{Desarrollo:} 

```{python sim1,echo=F,message=FALSE}
#!pip install tabulate
#!pip install matplotlib
import numpy as np
from tabulate import tabulate
import matplotlib.pyplot as plt
np.random.seed(1234)
N=10000
lambda_cons=1

xSim=np.random.exponential(scale=lambda_cons,size=N)
ySim=np.random.poisson(lam=xSim,size=N)
pySimTotal=np.unique(ySim,return_counts=True)
pySim=pySimTotal[1][:]*(1/N)
pyExact=list(map(lambda y:lambda_cons/((lambda_cons+1)**(y+1)),pySimTotal[0]))
diff=np.abs(pySim-pyExact)
mostrar=np.vstack( [pySimTotal[0],pySim,pyExact,diff])
print(tabulate(np.transpose(mostrar),headers=["y","Sim","Exact","Diff"]))
```

Veamos gráficamente este hecho


```{python sim2,results="hide"}
width=0.4
plt.bar(pySimTotal[0],pySim,width=width,label='Simulacion')
plt.bar(pySimTotal[0]+width,pyExact,width=width,label='Exacta')
plt.xticks()
plt.legend(loc='best')
plt.xlabel(r'$\theta$')
plt.ylabel(r'$p(\theta)$')
plt.show()
```


    
\begin{enumerate}
  \item[4.] Muestre que si $y\mid\theta$ tiene distribución exponencial con parámetro $\theta$, entonces la distribución $\textsf{Gamma}$ sirve como distribución previa conjugada para hacer inferencias sobre $\theta$, dada una muestra aleatoria de valores de $y$.
\end{enumerate}

\textbf{Desarrollo:}

Se tiene que $y_i\mid \theta \stackrel{\text{iid}}{\sim} exp(\theta)$ y la distribución previa $\theta \sim Gamma(a,b)$ entonces $$ p(y \mid \theta)=\theta e^{-\theta y} \quad \text{ y ademas} \quad p(\theta)=\frac{b^a}{\Gamma (a)}\theta^{a-1}e^{-b\theta}$$

así $s=\sum_{i=1}^ny_i$ estadístico suficiente  y la distribución a posterior sera $$p(\theta \mid \boldsymbol{y}) =  \stackrel{\text{T.Bayes}}{\frac{p(\boldsymbol{y}\mid \theta)p(\theta)}{p(\boldsymbol{y})}}$$  luego $$p(\boldsymbol{y})=\int_\Theta p(\boldsymbol{y},\theta) d\theta = \int_\Theta p(\boldsymbol{y} \mid \theta )p(\theta)d\theta =\int_0 ^\infty \theta^n e^{-\sum y_i \theta}[\frac{b^a}{\Gamma (a)}\theta^{a-1}e^{-b\theta}]d\theta=\frac{b^a}{\Gamma (a)}\int_0 ^\infty \theta^{(a+n)-1}e^{-(b+s)\theta}d\theta$$  
recordando la forma de la densidad de una distribución gamma completamos de la siguiente forma:

\begin{align}
p(\boldsymbol{y})=& \frac{b^a}{\Gamma (a)}\int_0 ^\infty \theta^{(a+n)-1}e^{-(b+s)\theta}d\theta \\
                =& \frac{b^a}{\Gamma (a)}\frac{\Gamma (a+n) }{(b+s)^{(a+n)}} \stackrel{\text{Densidad Gamma}}{ \int_0 ^\infty \frac{(b+s)^{(a+n)}} {\Gamma (a+n) }\theta^{(a+n)-1}e^{-(b+s)\theta}d\theta }  \\
                =& \frac{b^a}{\Gamma (a)}\frac{\Gamma (a+n) }{(b+s)^{(a+n)}} 
\end{align}

luego usando el teorema de Bayes

\begin{align}
  p(\theta \mid \boldsymbol{y}) = & \stackrel{\text{T.Bayes}}{\frac{p(\boldsymbol{y}\mid \theta)p(\theta)}{p(\boldsymbol{y})}} \\
                              = &\frac{\theta^n e^{-s\theta}\frac{b^a}{\Gamma (a)}\theta^{a-1}e^{-b\theta}}{\frac{b^a}{\Gamma (a)}\frac{\Gamma (a+n) }{(b+s)^{(a+n)}} } \quad \text{cancelando}\\
                              =&\frac{(b+s)^{(a+n)}}{\Gamma (a+n) }\theta^{(a+n)-1} e^{-(b+s) \theta}\\
                              \theta \mid \boldsymbol{y} \sim Gamma(a+n,b+s.)
\end{align}

entonces la distribución $\textsf{Gamma}$ sirve como distribución previa conjugada para hacer inferencias sobre $\theta$.

\begin{enumerate}
\item[5.]
Suponga que Su estado de información previo para $\theta$, la proporción de individuos que apoyan la pena de muerte en California, es Beta con media $E(\theta)$=0.6 y desviación estándar $DE(\theta)=0.3$.
  \begin{enumerate}
    \item Determine los hiperparámetros de Su distribución previa y dibuje la función de densidad correspondiente.
    
    \textbf{Desarrollo:} 
    
    Puesto que el estado de información previa se puede modelar a través de una función beta de parámetros a y b ,tendremos entonces que :
    
    \begin{equation*}
      E(\theta)=0.6 \Longrightarrow \frac{a}{a+b}=0.6 \Longrightarrow b=(2/3)a
    \end{equation*}
    de la misma manera
    \begin{align*}
        DE(\theta)&=(\frac{ab}{(a+b)^2(a+b+1)} )^{1/2} \Longrightarrow 0.09=\frac{ab}{(a+b)^2(a+b+1)}\\
                 0.09 &=\frac{(2/3)a^2}{(\frac{5}{3}a)^2(\frac{5}{3}a+1)}\\
               \frac{2}{3} &= (\frac{9}{100})(\frac{25}{9})(\frac{5}{3}a+1) \\
               \frac{8}{3} &=\frac{5}{3}a+1\\
               a&=1
    \end{align*}
    
    así $a=1$ y $b=2/3$ entonces el modelo será $Beta(1,\frac{2}{3})$
    

    
  \end{enumerate}
\end{enumerate}

```{python p5,}
from scipy import stats
import matplotlib.pyplot as plt
import numpy as np
#!pip install scipy
a=1
b=2/3
x=np.arange(0,1,0.001)
y=stats.beta.pdf(x,a,b)
plt.plot(x,y)
plt.title('Función de Masa de Probabilidad Beta a=1 , b=2/3')
plt.ylabel('probabilidad')
plt.xlabel('valores')
plt.show()
```
\begin{enumerate}
  \item[(b)] Se toma una muestra aleatoria de 1,000 californianos y el $65\%$ apoya la pena de muerte. Calcule tanto la media     como la desviación estándar posterior para $\theta$. Dibuje la función de densidad posterior correspondiente.
    
    \textbf{Solución:}
    
      Se tiene que la distribución muestral $s \mid \theta \sim binomial (n=1000,p=0.65)$ y $\theta\sim               beta(a=1,b=\frac{2}{3})$  por el modelo beta-binomial se tiene que la distribución posterior es $beta(1+650,1+350)$ donde $s=650$
      
    Además tenemos que el valor esperado es $E(s\mid \theta)= \frac{651}{651+351}=0.6497$  y la $Var(s\mid \theta)=\frac{(651)(351)}{(651+351)^2(651+351+1)}=0.0002269$ 
\end{enumerate}
```{python fig3,fig.width=1}
a=1
b=2/3
n=1000
s=650
ap=a+s
bp=b+n-s
mediaP=ap/(ap+bp) 
varP=(ap*bp)/((ap+bp)**2*(ap+bp+1))
xp=np.arange(0,1,0.001)
yp=stats.beta.pdf(x,ap,bp)
plt.plot(xp,yp)
plt.title("fmp posterior Beta (a=651 , b=350.66)")
plt.ylabel(r'$p(\theta \mid s) $')
plt.xlabel(r'$\theta $')
plt.show()
```